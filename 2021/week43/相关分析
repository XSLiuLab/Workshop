# 相关分析与回归分析

------

回归分析(Regression)：Dependant variable is defined and can be forecasted by independent variable.

相关分析(Correlation)：The relationship btw two variables. --- A dose not define or determine B.

![img](https://img-blog.csdnimg.cn/20200410180857484.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VtbWFsXw==,size_16,color_FFFFFF,t_70)

------

#### 相关分析

它用于量化两个连续变量之间的关联（例如，独立变量与因变量之间或两个独立变量之间）。回归分析是评估结果变量与一个或多个风险因素或变量之间关系的相关技术。结果变量也被称为因变量，风险因素被称为预测因子或解释性或自变量。在回归分析中，因变量表示为“y”，自变量表示为“ x”。

##### pearson相关分析

------

Pearson相关系数（皮尔森相关）是使用最广泛的相关性统计量，用于测量两组连续变量之间的线性关联程度。

Pearson相关系数应用于连续变量，假定两组变量均为正态分布、存在线性关系且等方差。线性关系假设两个变量之间是线性响应的，等方差假设数据在回归线上均匀分布。

~~~R
a<-data.frame(sample(1:10,8),sample(1:10,8))
cor.test(a[,1],a[,2],method="pearson")
~~~

##### spearman相关分析

------

Spearman秩相关系数（斯皮尔曼等级相关）是一种非参数统计量，其值与两组相关变量的具体值无关，而仅仅与其值之间的大小关系有关。Spearman秩相关依据两列成对等级的各对等级数之差进行计算，所以又称为“等级差数法”。当变量在至少是有序的尺度上测量时，它是合适的相关分析方法。

Spearman秩相关同样应用于连续变量，与Pearson相关相比Spearman秩相关不要求变量的正态性和等方差假设，且对异常值的敏感度较低（该方法基于变量的排序，因此异常值的秩次通常不会有明显变化），因此适用范围通常更广。但方法较为保守，统计效能较Pearson相关系数低，容易忽略一些不太强的线性关系。

~~~R
a<-data.frame(sample(1:10,8),sample(1:10,8))
cor.test(a[,1],a[,2],method="spearman")
~~~

##### 一致性分析

------

临床上的一致性检验指的在诊断试验中，研究者希望考察不同的研究方法在诊断结果上是否具有一致性。分为两种情况：一是评价待评价的诊断试验方法与金标准的一致性；二是评价两种化验方法对同一样本的化验结果的一致性或者两个医务工作者对同一组病人的诊断结论的一致性或者同一个医务工作者对同一组病人前后两次的观察做出的诊断的一致性等。

在R语言中有两个函数都可以进行一致性检验，分别是kappa检验和McNemar检验。当然，两者也是有一定的区别的。如果检验的项目是多等级的分布那么选择kappa检验；如果是2个等级的检查那么选择McNemar检验。

~~~R
require(irr)
data(diagnoses)
dat=diagnoses[,c(1,2)]
kappa2(dat[,c(1,2)],'unweighted') ##Weight参数是函数的重点，如果有多个检查项目中有一个是为0的时候需要加权检验，其他时候一般都是非加权检验。
~~~

~~~R
Performance <-matrix(c(794, 86, 150, 570),nrow = 2,dimnames = list("1st Survey" = c("Approve","Disapprove"),
"2nd Survey" =c("Approve", "Disapprove")))
Performance
mcnemar.test(Performance)##其中一个参数correct默认是true。我们通过理论频数进行判断。所谓理论频数指某H0假设计算各分类理论上的发生或者未发生计数值，记为T。如果某个格子出现1≤T ≤5，则需作连续性校正。
~~~

#### 回归分析

------

回归分析（英语：Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变数间是否相关、相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。

##### 线性回归

------

回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

~~~R
lm(formula,data)
x <- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131)
y <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)
#
summary(lm(y~x))
~~~

##### Logistic回归

------

在日常学习或工作中经常会使用线性回归模型对某一事物进行预测，例如预测房价、身高、GDP、学生成绩等，发现这些被预测的变量都属于连续型变量。然而有些情况下，被预测变量可能是二元变量，即成功或失败、流失或不流失、涨或跌等，对于这类问题，线性回归将束手无策。这个时候就需要另一种回归方法进行预测，即Logistic回归

~~~R
fit = glm(churn ~ .,data = trainset,family = binomial)
~~~

**有时，我们会发现有很多自变量，或者自变量之间高度相关，所以就需要挑选变量来构建合适的模型。R软件提供了非常方便地进行逐步回归分析的计算函数step(),它是以AIC信息统计量为准则，通过选择最小的AIC信息统计量。来达到提出或添加变量的目的。**

~~~R
m<-read.table("m.tsv")
lm(m$V55~m$copyV3+m$copyV4+m$copyV5+m$copyV6)

##############################逐步回归
 a<-lm(m$V55~m$copyV3+m$copyV4+m$copyV5+m$copyV6)
 a_step<-step(a,direction = "both")
 summary(a_step)

############################ 多变量
h<-colnames(m)
w<-str_c(h[length(h)],h[2],sep="~")
for(v in 3:(length(h)-2)){
    w<-str_c(w,colnames(m)[v],sep="+")
}
o<-lm(substitute(j,list(j=w)),data=m)
a<-step(o,direction="both")
summary(a)
~~~

因此我还是推荐最好根据相关性手动删除高相关的变量。

### 数据分析必须警惕的坑：辛普森悖论（Simpson's paradox）

比如，在总样本中，A与B呈正相关；但是拆分成多个亚组之后，它们之间的关系却变成了负相关！即，在总样本和亚组中，A与B之间的关系是完全相反的！

~~~R
# install.packages("ggplot2")
# install.packages("correlation")
library(correlation)
library(ggplot2)
mydata <- simulate_simpson(n = 100, groups = 5, r = 0.5, difference = 1) ###simulate_simpon()可模拟出一组两两相关的数据，并可以通过修改上述代码中的几个参数创建想要的数据，比如样本量，组别的数量，相关系数等。
summary(mydata)

ggplot(mydata, aes(x = V1, y = V2)) +
  geom_point(aes(color = Group), alpha = 0.3) +
  geom_smooth(aes(color = Group), method = "lm") +
  theme_classic()

ggplot(mydata, aes(x = V1, y = V2)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = "lm", color = "lightseagreen", size = 1.5) + 
  theme_classic()
~~~

举一个简单的例子，假设我们有50岁以上和50岁以下两组患者，在收集了他们的每周运动小时数和病发风险后，我们得到了下面两幅有关运动和病情恶化几率关系的图表：

![preview](https://pic3.zhimg.com/v2-ae7c2f946d195c2e237e6affcff72de6_r.jpg)



上图很清楚地表明两者是负相关的，每周运动得越久，患者病情恶化的可能性就更低。但是，如果我们把两组数据结合在一起：

![img](https://pic3.zhimg.com/80/v2-44614349d6fc1d9a2b8a124189ee53ba_720w.jpg)





很明显，运动肯定不是影响病情加重的唯一因素，饮食、环境、遗传……它的影响因素非常复杂。但在上图中，我们只看到了恶化几率和运动时长之间的关系，在没有控制变量的情况下，这相当于假设恶化只是由运动引起的，显然不合理。

为了避免辛普森悖论，我们事先要确定数据的统计方法，是合并更合理，还是分组更合理——**这取决于数据生成的过程，即数据的因果模型。**

辛普森悖论非常重要，因为它时刻在提醒我们，**表格中显示的数据可能并不是所有数据**。我们不能只满足于数字、数据，而必须关注数据的生成过程  ——因果模型——对数据负责。在大学里，对因果关系的思考并不是大多数数据科学家会在课上学到的技能，但是这能有效防止我们从数字中得出错误结论。一个真正好的数据科学家不仅是数据分析上的专家，他也能结合专业领域的知识，做出更好的决策。



#### REFERCENE

————————————————
1，版权声明：本文为CSDN博主「陆嵩」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/lusongno1/article/details/77239829————————————————
2，版权声明：本文为CSDN博主「拓端研究室」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_19600291/article/details/102970980————————————————
3，版权声明：本文为CSDN博主「weixin_39845461」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_39845461/article/details/112560522

4，https://cloud.tencent.com/developer/article/1476390

5，https://zhuanlan.zhihu.com/p/47339109
